{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMKrKUNeOvNsvlwg32ilwK+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sakshi-Prajapati/ML_LABS/blob/main/ML_LAB7.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mkueYgR_PV5V"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import io\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from copy import deepcopy\n",
        "import tensorflow as tf\n",
        "\n",
        "from math import exp\n",
        "location=\"/content/BuyComputer.csv\"\n",
        "data = pd.read_csv(location, encoding=\"latin\")\n",
        "data.drop(columns=['User ID',],axis=1,inplace=True)\n",
        "# print(data)\n",
        "\n",
        "x=data[data.columns[:-1]]\n",
        "y=data[data.columns[-1]]\n",
        "\n",
        "sc=StandardScaler()\n",
        "x=sc.fit_transform(x)\n",
        "\n",
        "x_train,x_test,y_train,y_test=train_test_split(x,y,test_size = 0.20, random_state =112)\n",
        "\n",
        "total_features=len(x_train[0])\n",
        "\n",
        "entries=x_train[:,0]\n",
        "\n",
        "# print(len(x_train),len(entries))\n",
        "\n",
        "\n",
        "x_train=np.array(x_train)\n",
        "y_train=np.array(y_train)\n",
        "# print(input)\n",
        "weights=np.array([[2.] for _ in range(total_features)])\n",
        "bias=np.array([[0.2] for _ in range(len(x_train))])\n",
        "\n",
        "x_train=torch.tensor(x_train,requires_grad=False)\n",
        "y_train=torch.tensor(y_train,requires_grad=False)\n",
        "\n",
        "iterations=1\n",
        "for _ in range(iterations):\n",
        "  weights=torch.tensor(weights,requires_grad=True)\n",
        "  bias=torch.tensor(bias,requires_grad=True)\n",
        "\n",
        "  z=torch.matmul(x_train,weights)+bias\n",
        "  z1=torch.exp(-z)\n",
        "  denom=torch.add(z1,1)\n",
        "  y_pred=denom.clone()\n",
        "  y_pred.pow_(-1)\n",
        "  y_pred=y_pred.round()\n",
        "  # print(y_pred)\n",
        "  print(torch.log(y_pred))\n",
        "  # loss=(y_train*torch.log(y_pred))+(torch.add(1.,-y_train)*torch.log(1-y_pred))\n",
        "  first=torch.mul(y_train,torch.log(y_pred))\n",
        "  second=torch.mul(torch.sub(1.,y_train),torch.log(torch.sub(1.,torch.log(y_pred))))\n",
        "  loss=torch.add(first,second)\n",
        "  print(loss)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.python.ops.array_ops import zeros_like_v2\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import io\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import log_loss\n",
        "import tensorflow as tf\n",
        "\n",
        "from math import exp\n",
        "\n",
        "learning_rate=0.005\n",
        "\n",
        "def convert(arr):\n",
        "  return [[elem] for elem in arr]\n",
        "\n",
        "def deconvert(arr):\n",
        "  return [elem[0] for elem in arr]\n",
        "\n",
        "def sigmoid(z):\n",
        "  denom=1+np.exp(-z)\n",
        "  value=1/denom\n",
        "  return value\n",
        "  pass\n",
        "\n",
        "def predict(input,weights,bias):\n",
        "  z=np.dot(input,weights)\n",
        "  z+=bias\n",
        "  pred=sigmoid(z)\n",
        "  return pred\n",
        "\n",
        "def predict_bulk(input,weights,bias):\n",
        "  pred=[]\n",
        "  for sample in input:\n",
        "    pred_i=predict(sample,weights,bias)\n",
        "    pred.append(pred_i)\n",
        "  pred=np.array(pred)\n",
        "  pred=np.round(pred)\n",
        "\n",
        "  return pred\n",
        "  pass\n",
        "\n",
        "def partial_derivate(pred,output,x):\n",
        "  first=(pred-output)\n",
        "  x=convert(x)\n",
        "  der_all=first*x\n",
        "  \n",
        "  der_all=deconvert(der_all)\n",
        "  der_value=sum(der_all)/len(der_all)\n",
        "  return der_value\n",
        "  pass\n",
        "\n",
        "def param_update(features,pred,output,params):\n",
        "  updated_params=[]\n",
        "  for i in range(len(params)):\n",
        "    x_i=np.array(features[:,i])\n",
        "    d_i=partial_derivate(pred,output,x_i)\n",
        "    deduct=learning_rate*d_i\n",
        "    curr_param=params[i]\n",
        "    curr_param=curr_param-deduct\n",
        "    updated_params.append(curr_param)\n",
        "  return updated_params\n",
        "  pass\n",
        "\n",
        "\n",
        "def calculate_loss(pred,output):\n",
        "  # remaining to calculate\n",
        "  pass\n",
        "\n",
        "def calculate_accuracy(y_pred,y_act):\n",
        "  cmat=confusion_matrix(y_pred,y_act)\n",
        "  # print(cmat)\n",
        "  truth=sum(cmat[i][i] for i in range(len(cmat)))\n",
        "  total=0\n",
        "  for row in cmat:\n",
        "    for elem in row:\n",
        "      total+=elem\n",
        "  return truth/total\n",
        "\n",
        "location=\"/content/BuyComputer.csv\"\n",
        "data = pd.read_csv(location, encoding=\"latin\")\n",
        "data.drop(columns=['User ID',],axis=1,inplace=True)\n",
        "\n",
        "x=data[data.columns[:-1]]\n",
        "y=data[data.columns[-1]]\n",
        "\n",
        "sc=StandardScaler()\n",
        "x=sc.fit_transform(x)\n",
        "\n",
        "x_train,x_test,y_train,y_test=train_test_split(x,y,test_size = 0.20, random_state =112)\n",
        "\n",
        "total_features=len(x_train[0])\n",
        "\n",
        "entries=x_train[:,0]\n",
        "\n",
        "x_train=np.array(x_train)\n",
        "\n",
        "\n",
        "y_train=convert(y_train)\n",
        "y_train=np.array(y_train)\n",
        "\n",
        "y_test=np.array(convert(y_test))\n",
        "\n",
        "\n",
        "weights=np.array([[2.] for _ in range(total_features)])\n",
        "bias=np.array([0.10])\n",
        "\n",
        "iterations=1000\n",
        "accuracy=0\n",
        "loss_list = []\n",
        "for _ in range(iterations):\n",
        "  pred=predict_bulk(x_train,weights,bias)\n",
        "  weights,bias=param_update(x_train,pred,y_train,[weights,bias])\n",
        "  c_accuracy=calculate_accuracy(pred,y_train)\n",
        "  accuracy=max(accuracy,c_accuracy)\n",
        "  loss = log_loss(y_train,pred)\n",
        "  loss_list.append(loss)\n",
        "\n",
        "print(accuracy)\n",
        "\n",
        "# testing model\n",
        "y_pred=predict_bulk(x_test,weights,bias)\n",
        "test_accuracy=calculate_accuracy(y_pred,y_test)\n",
        "print(test_accuracy)\n",
        "\n",
        "# Plot training loss\n",
        "# plt.plot(loss_list)\n",
        "# plt.title('Training Loss')\n",
        "# plt.xlabel('Epochs')\n",
        "# plt.ylabel\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ywd8TwmWQg5D",
        "outputId": "674096d6-06d8-4c74-b320-28977ecccd10"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.80625\n",
            "0.8375\n"
          ]
        }
      ]
    }
  ]
}